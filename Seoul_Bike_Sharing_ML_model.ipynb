{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "w6K7xa23Elo4",
        "mDgbUHAGgjLW",
        "HhfV-JJviCcP",
        "nA9Y7ga8ng1Z",
        "dauF4eBmngu3",
        "GF8Ens_Soomf",
        "x-EpHcCOp1ci",
        "n3dbpmDWp1ck",
        "NC_X3p0fY2L0",
        "q29F0dvdveiT",
        "g-ATYxFrGrvw",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "49K5P_iCpZyH",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "7wuGOrhz0itI",
        "578E2V7j08f6",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "yiiVWRdJDDil",
        "qjKvONjwE8ra",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "EyNgTHvd2WFk",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/naman39910/Exploratory-Data-Analysis/blob/main/Seoul_Bike_Sharing_ML_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    -\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - EDA/Regression\n",
        "##### **Contribution**    - Individual\n",
        "##### **Team Member 1 -**\n",
        "##### **Team Member 2 -**\n",
        "##### **Team Member 3 -**\n",
        "##### **Team Member 4 -**"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "9sIEUrVEQNai"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project focuses on analyzing the Seoul Bike Sharing Dataset to gain a comprehensive understanding of the factors that influence the number of rented bikes. The dataset, sourced from \"/content/SeoulBikeData.csv\", contains detailed hourly information on bike rental counts in Seoul, alongside a variety of related features. These features include crucial environmental factors such as temperature, humidity, wind speed, visibility, rainfall, and snowfall, as well as temporal information like the date, hour, and seasonal indicators. Additionally, the dataset includes sunshine duration, which could play a significant role in bike usage patterns.\n",
        "\n",
        "The primary objective of this project is to conduct an in-depth exploratory data analysis (EDA) to uncover underlying patterns, trends, and relationships within the data. This involves visualizing the distribution of bike rental counts, examining how different environmental and temporal factors correlate with rental demand, and identifying any anomalies or interesting observations. For instance, we will investigate how bike rentals vary across different hours of the day, days of the week, and seasons of the year. We will also explore the impact of weather conditions, such as temperature, rainfall, and sunshine, on the number of bikes rented.\n",
        "\n",
        "Beyond the exploratory analysis, the project aims to leverage the insights gained to potentially build a regression model. This model will be designed to predict hourly bike rental demand based on the available features. The development of such a predictive model involves several steps, including feature engineering, where we might create new features from existing ones (e.g., combining date and hour to represent specific time periods), handling missing values if any exist (although the initial data inspection suggests none), addressing potential outliers, and encoding categorical variables appropriately.\n",
        "\n",
        "Before model building, we will also consider data transformation and scaling techniques to ensure the data is in a suitable format for the chosen regression algorithm. The dataset will then be split into training and testing sets to train and evaluate the model's performance. Various regression algorithms will be considered, and their performance will be evaluated using appropriate metrics such as Mean Absolute Error (MAE), Mean Squared Error (MSE), and R-squared. Cross-validation and hyperparameter tuning techniques will be employed to optimize the chosen model and improve its predictive accuracy.\n",
        "\n",
        "The insights derived from both the EDA and the regression model will be crucial for providing actionable recommendations to optimize bike allocation and management in Seoul. For example, understanding the peak hours and seasons for bike rentals can help the operating company ensure sufficient bike availability during these times. Similarly, knowing how weather conditions affect demand can inform strategies for adjusting bike distribution or offering incentives during unfavorable weather. The project aims to deliver a comprehensive analysis that not only explains the factors influencing bike rentals but also provides practical insights for improving the efficiency and effectiveness of the Seoul Bike Sharing program, ultimately contributing to better urban mobility and sustainability. The project adheres to general guidelines for structured code, proper documentation, and the inclusion of meaningful visualizations and model explanations to ensure clarity and reproducibility."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Currently Rental bikes are introduced in many urban cities. The business problem is to ensure a stable supply of rental bikes in urban cities by predicting the demand for bikes at each hour. By providing a stable supply of rental bikes, the system can enhance mobility comfort for the public and reduce waiting time, leading to greater customer satisfaction.\n",
        "\n",
        "To address this problem, i need to develop a predictive model that takes into account various factors that may influence demand, such as time of day, seasonality, weather conditions, and holidays. By accurately predicting demand, the bike sharing system operators can ensure that there is an adequate supply of bikes available at all times, which can improve the user experience and increase usage of the bike sharing system. This can have a positive impact on the sustainability of urban transportation, as it can reduce congestion, air pollution, and greenhouse gas emissions."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime as dt\n",
        "\n",
        "\n",
        "# Import Visualization Libraries\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Import warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Import preporcessing libraries\n",
        "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
        "\n",
        "# Import model selection libraries\n",
        "from sklearn.model_selection import train_test_split,GridSearchCV,RandomizedSearchCV\n",
        "\n",
        "# Import Outlier influence library\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "# Import Model\n",
        "from sklearn.linear_model import LinearRegression,Lasso,Ridge\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor\n",
        "from xgboost import XGBRegressor\n",
        "import xgboost as xgb\n",
        "\n",
        "# Import evaluation metric libraries\n",
        "from sklearn.metrics import mean_squared_error,r2_score,mean_absolute_error\n",
        "\n",
        "# Import tree for visualization\n",
        "from sklearn.tree import export_graphviz\n",
        "from sklearn import tree\n",
        "from IPython.display import SVG,display\n",
        "from graphviz import Source\n"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "dataset_url= \"/content/SeoulBikeData.csv\"\n",
        "dataset = pd.read_csv(dataset_url, encoding='ISO-8859-1')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "dataset.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "dataset.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "dataset.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "dataset.duplicated().sum()"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "dataset.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "sns.heatmap(dataset.isnull(), cbar=False)"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this dataset we know that these things-\n",
        "\n",
        "1. Number of Entries: The dataset consists of 28242 entries, ranging\n",
        "from index 0 to 28241.\n",
        "\n",
        "2. Columns & Rows: There are 14 columns  and 8760 rows in the dataset.\n",
        "\n",
        "3. Data Types:\n",
        "Most of the columns (6 out of 8) are of the int64 & float64 data type.\n",
        "Only the Item and Area columns are of the object data type.\n",
        "\n",
        "4. Missing Values: There doesn't appear to be any missing values in the dataset as each column has 28242 non-null entries."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "dataset.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "dataset.describe()\n"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset contains weather information (Temperature, Humidity, Windspeed, Visibility, Dewpoint, Solar radiation, Snowfall, Rainfall), the number of bikes rented per hour and date information.\n",
        "Attribute Information :\n",
        "\n",
        "Date : The date of the observation.\n",
        "\n",
        "Rented Bike Count : The number of bikes rented during the observation period.\n",
        "\n",
        "Hour : The hour of the day when the observation was taken.\n",
        "\n",
        "Temperature(°C) : The temperature in Celsius at the time of observation.\n",
        "\n",
        "Humidity(%) : The percentage of humidity at the time of observation.\n",
        "\n",
        "Wind speed (m/s) : The wind speed in meters per second at the time of observation.\n",
        "\n",
        "Visibility (10m) : The visibility in meters at the time of observation.\n",
        "\n",
        "Dew point temperature(°C) : The dew point temperature in Celsius at the time of observation.\n",
        "\n",
        "Solar Radiation (MJ/m2) : The amount of solar radiation in mega-joules per square meter at the time of observation.\n",
        "\n",
        "Rainfall(mm) : The amount of rainfall in millimeters during the observation period.\n",
        "\n",
        "Snowfall(cm) : The amount of snowfall in centimeters during the observation period.\n",
        "\n",
        "Seasons : The season of the year when the observation was taken.\n",
        "\n",
        "Holiday : Whether the observation was taken on a holiday or not.\n",
        "\n",
        "Functioning Day : Whether the bike sharing system was operating normally or not during the observation period."
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "dataset.nunique()"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# converting date variable into datetime datatype\n",
        "dataset['Date'] = dataset['Date'].apply(lambda x: dt.strptime(x, '%d/%m/%Y'))"
      ],
      "metadata": {
        "id": "KxhY7pkTcySG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating new columns for day and month\n",
        "dataset['Day of week'] = dataset['Date'].apply(lambda x : x.isoweekday())\n",
        "dataset['Month'] = dataset['Date'].apply(lambda x : x.month)"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# engineering new feature 'weekend' from day_of_week\n",
        "dataset['weekend'] = dataset['Day of week'].apply(lambda x: 1 if x>5 else 0)"
      ],
      "metadata": {
        "id": "810hLKJPdTqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.head()"
      ],
      "metadata": {
        "id": "50kxdl-ReNoF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# defining continuous independent variables separately\n",
        "cont_var = ['Temperature(°C)', 'Humidity(%)', 'Wind speed (m/s)', 'Visibility (10m)', 'Dew point temperature(°C)', 'Solar Radiation (MJ/m2)', 'Rainfall(mm)', 'Snowfall (cm)']"
      ],
      "metadata": {
        "id": "SLTLL9_bgGHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# defining dependent variable\n",
        "dependent_variable = ['Rented Bike Count']"
      ],
      "metadata": {
        "id": "82MoyUYCgmgh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# defining categorical independent variables separately\n",
        "cat_var = ['Hour', 'Seasons', 'Holiday', 'Functioning Day','Month', 'Day of week', 'weekend']"
      ],
      "metadata": {
        "id": "3yDcW-AJg2Ef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the Date column, 'month' and 'day of the week' columns are created.\n",
        "\n",
        "From the 'day of the week' column, 'weekend' column is created where 6 and 7 are the weekends (Saturday and Sunday).\n",
        "\n",
        "We have also defined the continuous variables, dependent variable and categorical variables for ease of plotting graphs."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1 : Dependent variable Distribution"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code for distribution of target variable\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.displot(dataset['Rented Bike Count'])\n",
        "plt.title('Distribution of Rented Bike Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "A distplot, also known as a histogram-kernel density estimate (KDE) plot. It is useful because it provides a quick and easy way to check the distribution of the data, identify patterns or outliers, and compare the distribution of multiple variables. It also allows to check if the data is following normal distribution or not.\n",
        "\n",
        "Thus, I used the histogram plot to analyse the variable distributions over the whole dataset whether it's symmetric or not."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "From above distribution plot of dependent variable rented bike, we can clearly see that the distribution is positively skewed (Right skewed).\n",
        "\n",
        "It means that distribution is not symmetric around the the mean."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Yes, definately from this insight we got to know that our data is not normally distributed. So, before doing or implementing any model on this data we need to normalise this data."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2 : Distribution/Box plot"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# visualizing code of histogram plot and box plot for each columns to know the distribution of the dataset\n",
        "for i in dataset.columns:\n",
        "    fig,axes = plt.subplots(nrows=1,ncols=2,figsize=(13,4))\n",
        "    sns.histplot(dataset[i], ax = axes[0],kde=True)\n",
        "    sns.boxplot(dataset[i], ax = axes[1], orient='h',showmeans=True,color='pink')\n",
        "    fig.suptitle(\"Distribution plot of \"+ i, fontsize = 12)\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A histplot is a type of chart that displays the distribution of a dataset. It is a graphical representation of the data that shows how often each value or group of values occurs. Histplots are useful for understanding the distribution of a dataset and identifying patterns or trends in the data. It is also useful when dealing with large data sets (greater than 100 observations). It can help detect any unusual observations (outliers) or any gaps in the data.\n",
        "\n",
        "Thus, we used the histogram plot to analysis the variable distributions over the whole dataset whether it's symmetric or not.\n",
        "\n",
        "A boxplot is used to summarize the key statistical characteristics of a dataset, including the median, quartiles, and range, in a single plot. Boxplots are useful for identifying the presence of outliers in a dataset, comparing the distribution of multiple datasets, and understanding the dispersion of the data. They are often used in statistical analysis and data visualization.\n",
        "\n",
        "Thus, for each numerical varibale in the given dataset, we used box plot to analyse the outliers and interquartile range including mean, median, maximum and minimum value."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From above univariate analysis of all continuous feature variables. We got to know that only tempture and humidity columns are looks normally distributed others shows the different distributions.\n",
        "\n",
        "Also we can see that there are outlier values in snowfall, rainfall, wind speed & solar radiation columns"
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Histogram and Box plot cannot give us whole information regarding data. It's done just to see the distribution of the column data over the dataset."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3 : Dependent variable with continuous variables (Bivariate)"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyzing the relationship between the dependent variable and the continuous variables\n",
        "for i in cont_var:\n",
        "    plt.figure(figsize=(8,6))\n",
        "    sns.relplot(x=i,y= dependent_variable[0],data=dataset,kind='scatter')\n",
        "    plt.title(f'Relationship between {dependent_variable[0]} and {i}')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regplot is used to create a scatter plot with linear regression line. The purpose of this function is to visualize the relationship between two continuous variables. It can help to identify patterns and trends in the data, and can also be used to test for linearity and independence of the variables.\n",
        "\n",
        "To check the patterns between independent variable with our rented bike dependent variable we used this regplot."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From above regression plot we can see that there is some linearity between temperature, solar radiation & dew point temperature with dependent variable rented bike\n",
        "\n",
        "Other variables are not showing any patterns."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Yes, it helped a little bit from this we got to know that there are few variables which are showing some patterns with dependent variable this variable are maybe important feature while predicting for rented bike count so business needs focus on these variables."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4 : Categorical variables with dependent variable(Bivariate)"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyzing the relationship between the dependent variable and the categorical variables\n",
        "for i in cat_var:\n",
        "    plt.figure(figsize=(8,6))\n",
        "    sns.barplot(x=i,y= dependent_variable[0],data=dataset)\n",
        "    plt.title(f'Relationship between {dependent_variable[0]} and {i}')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bar charts are used to compare the size or frequency of different categories or groups of data. Bar charts are useful for comparing data across different categories, and they can be used to display a large amount of data in a small space.\n",
        "\n",
        "To show the distribution of the rented bike count with other categorical variables we used bar charts."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From above bar charts we got insights:\n",
        "\n",
        "1. In hour vs rented bike chart there is high demand in the morning 8'o clock and evening 18'o clock.\n",
        "\n",
        "2. From season vs rented bike chart there is more demand in summer and less demand in winter.\n",
        "From day_of_week vs rented bike chart there is high demand on working days.\n",
        "\n",
        "3. From month chart we know that there is high demand in mont"
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Yes, this insights are going to provide some positive business impact, beacause analysing the demand on the basis of categorical varible we got to know that when demand for bike is more so we can focus more on that portion."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5 : Rented Bike vs Hour"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ploting line graph\n",
        "avg_rent_per_hour = dataset.groupby('Hour')['Rented Bike Count'].mean()\n",
        "\n",
        "# plot average rent over time(hrs)\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.lineplot(x=avg_rent_per_hour.index, y=avg_rent_per_hour.values)\n",
        "plt.xlabel('Hour')\n",
        "plt.ylabel('Rented Bike Count')\n",
        "plt.title('Average Rented Bike Count Over Time')"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A line plot, also known as a line chart or line graph, is a way to visualize the trend of a single variable over time. It uses a series of data points connected by a line to show how the value of the variable changes over time.\n",
        "\n",
        "Line plots are useful because they can quickly and easily show trends and patterns in the data. They are particularly useful for showing how a variable changes over a period of time. They are also useful for comparing the trends of multiple variables.\n",
        "\n",
        "To see how rented bike demand is distributed over 24 hours time we used line plot."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From above line plot we can clearly see that there is high demand in the morning and in the evening."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, from above insight we know that there is high demand in morning and evening so business needs to focus more on that time slot, as well as try to meet the demand on that time slot."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6 Bike demand throughout the day (Multivariate)"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ploting point plot\n",
        "for i in cat_var:\n",
        "    if i == 'Hour':\n",
        "        continue\n",
        "    else:\n",
        "        fig, ax = plt.subplots(figsize=(10,6))\n",
        "        sns.pointplot(x= 'Hour',y='Rented Bike Count',data=dataset, hue =i)\n",
        "        plt.title(f'Bike demand throughout the day for {i}')\n",
        "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left',title=i)\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "XRHn_eIys1aR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "A line plot, also known as a line chart or line graph, is a way to visualize the trend of a single variable over time. It uses a series of data points connected by a line to show how the value of the variable changes over time.\n",
        "\n",
        "Line plots are useful because they can quickly and easily show trends and patterns in the data. They are particularly useful for showing how a variable changes over a period of time. They are also useful for comparing the trends of multiple variables.\n",
        "\n",
        "To show the demand of rented bike throughout the day on the basis of other categorical variable we used line plot drawing multiple lines on charts."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "From above line plots we see that :\n",
        "\n",
        "1. In winter season there is no significant demand even in the morning or in the evening.\n",
        "\n",
        "2. On the functional day (i.e No Holiday) there is spike in morning and in evening, but that is not there on Holidays.\n",
        "\n",
        "3. Around 3 months in winter season (i.e December, January & February) there is low demand.\n",
        "\n",
        "4. On weekend almost throught the day there is demand."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, from this analysis we figure out some key factors such as high demand in morning and evening slot in all the seasons."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7 : Pie plot seasons"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "winter_data = dataset[dataset['Seasons'] == 'Winter']['Rented Bike Count'].sum()\n",
        "spring_data = dataset[dataset['Seasons'] == 'Spring']['Rented Bike Count'].sum()\n",
        "summer_data = dataset[dataset['Seasons'] == 'Summer']['Rented Bike Count'].sum()\n",
        "autumn_data = dataset[dataset['Seasons'] == 'Autumn']['Rented Bike Count'].sum()\n",
        "\n",
        "Bike_Seasons = [winter_data, spring_data, summer_data, autumn_data]\n",
        "Seasons = ['Winter','Spring','Summer','Autumn']\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.pie(Bike_Seasons, labels=Seasons, autopct='%1.1f%%', startangle=140)\n",
        "plt.title('Distribution of Rented Bike Count by Season')\n",
        "plt.axis('equal') # Equal aspect ratio ensures that pie is drawn as a circle.\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Pie charts are generally used to show the proportions of a whole, and are especially useful for displaying data that has already been calculated as a percentage of the whole.\n",
        "\n",
        "So, we used pie chart to see percentage distribution of rented bike on the basis of seasonsAnswer Here."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "From above pie chart:\n",
        "\n",
        "1. In year data season summer contributes around 36% then autumn around 29%\n",
        "\n",
        "2. Lowest demand in winter, it contributes around only 7%"
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This insights only tell about percentage contribution of year data of season variable, which clearly gave indication about demand."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8  : Categorical plot for seasons"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "sns.catplot(x='Seasons',y='Rented Bike Count',data=dataset,kind='box')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Catplot is used to create a categorical plot. Categorical plots are plots that are used to visualize the distribution of a categorical variable. They can be used to show how a variable is related to a categorical variable and can also be used to compare the distribution of multiple categorical variables.\n",
        "\n",
        "To see the distribution of the rented bike on basis of season column we used catplot."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From above catplot we got to know that:\n",
        "\n",
        "1. There is low demand in winter\n",
        "\n",
        "2. Also in all seasons upto the 2500 bike counts distribution is seen dense."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Yes, from this catplot we know that there is high bike count upto the 2500 so, above that there maybe outliers present. business needs to evaluate that.Answer Here"
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9 : Avg Rented Bike Count by Wind speed"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.lineplot(x='Wind speed (m/s)',y='Rented Bike Count',data=dataset)\n",
        "plt.title('Avg Rented Bike Count by Wind speed')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Line charts are a useful tool for visualizing trends over time. It allows us in easy identification of patterns and changes over time (in this case over wind speed).Answer Here."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Line charts are a useful tool for visualizing trends over time. It allows us in easy identification of patterns and changes over time (in this case over wind speed)."
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This may not be much helpful in creating positive business impact as\n",
        "this is a natural phenomenon and we can't control it."
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10 : Avg Rented Bike Count by Humidity"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.lineplot(x='Humidity(%)',y='Rented Bike Count',data=dataset)\n",
        "plt.title('Avg Rented Bike Count by Humidity')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Line charts are a useful tool for visualizing trends over time. It allows us in easy identification of patterns and changes over time (in this case over humidity)."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After certain level as humidity increases demand decreases as too much humidity may generally caused due to rain or snowfall as we already saw they leads to decrease in demand."
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This may not be much helpful in creating positive business impact as\n",
        "this is a natural phenomenon and we can't control it."
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1 : Avg Rented Bike Count by Visibility"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.lineplot(x='Visibility (10m)',y='Rented Bike Count',data=dataset)\n",
        "plt.title('Avg Rented Bike Count by Visibility')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Line charts are a useful tool for visualizing trends over time. It allows us in easy identification of patterns and changes over time (in this case over wind speed)."
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After certain level as visibility lower the rental bike deman has decrease due to the safty concerns and unpleasant riding and in higher visibility incerese the jrental bike demand.\n",
        "\n",
        "and also huge fluctuations in solar visibility may be caused due to day-night cycle as there is no sunlight at night time."
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This may not be much helpful in creating positive business impact as this is a natural phenomenon and we can't control it."
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11 : Temprature over time"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 visualization code\n",
        "plt.subplot(2, 1, 1)\n",
        "sns.lineplot(x='Date',y='Temperature(°C)',hue = 'Month',data=dataset,color='red')\n",
        "plt.title(\"Temperature by Date for each Month\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Line charts are a useful tool for visualizing trends over time. It allows us in easy identification of patterns and changes over time."
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As expected temperature rises during summer months and lowers in winter months."
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This may not be much helpful in creating positive business impact as this is a natural phenomenon and we can't control it."
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12 Solar radiation over time"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 12 visualization code\n",
        "plt.subplot(2, 1, 1)\n",
        "sns.lineplot(x='Date',y='Solar Radiation (MJ/m2)',hue = 'Month',data=dataset,color='red')\n",
        "plt.title(\"Solar Radiation by Date for each Month\")"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Line charts are a useful tool for visualizing trends over time. It allows us in easy identification of patterns and changes over time.Answer Here."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "solar radiation is more in summer months compared to winter months, also huge fluctuations in solar radiation may be caused due to day-night cycle as there is no sunlight at night time."
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This may not be much helpful in creating positive business impact as this is a natural phenomenon and we can't control it."
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "numeric_dataset = dataset.select_dtypes(include=np.number)\n",
        "corr = numeric_dataset.corr()\n",
        "mask = np.zeros_like(corr)\n",
        "\n",
        "mask[np.triu_indices_from(mask)] = True\n",
        "\n",
        "with sns.axes_style(\"white\"):\n",
        "    f, ax = plt.subplots(figsize=(14,7))\n",
        "    ax = sns.heatmap(corr , mask=mask, vmin = -1,vmax=1, annot = True, cmap=\"YlGnBu\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The correlation coefficient is a measure of the strength and direction of a linear relationship between two variables. A correlation matrix is used to summarize the relationships among a set of variables and is an important tool for data exploration and for selecting which variables to include in a model. The range of correlation is [-1,1].\n",
        "\n",
        "Thus to know the correlation between all the variables along with the correlation coeficients, we have used correlation heatmap."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From above correlation map we can clearly see that:\n",
        "\n",
        "1. There is high multicolinearity between independent variable (i.e temperature & dew point temp, humidity & dew point temp, weekend & day of week).\n",
        "2. There is correlation of temperature, hour, dew point temp & solar radiation with dependent variable rented bike.\n",
        "3. Other than that we didnt see any correlation."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "sns.pairplot(dataset)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A pairplot, also known as a scatterplot matrix, is a visualization that allows you to visualize the relationships between all pairs of variables in a dataset. It is a useful tool for data exploration because it allows you to quickly see how all of the variables in a dataset are related to one another.\n",
        "\n",
        "Thus, we used pair plot to analyse the patterns of data and realationship between the features. It's exactly same as the correlation map but here you will get the graphical representation."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From above pair plot we got to know that, there is not clear linear relationship between variables. other than dew point temp, temperature & solar radiation there is not any relationship."
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Based on above chart experiments i have noticed that our dependent variable (Rented Bike Demand) does not seems to normally distributed so i have made hypothetical assumption that our data is normally distributed and for that i have decided to do statistical analysis.\n",
        "\n",
        "1. Rented Bike Demand in hot weather is higher compared to demand in cold weather.\n",
        "\n",
        "2. Rented Bike Demand during rush hour (7-9AM & 5-7PM) and non-rush hour are different.\n",
        "\n",
        "3. Average Rented Bike Demand is different in different seasons."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1\n",
        "\n",
        "Rented Bike Demand in hot weather is higher compared to demand in cold weather."
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis: H₀:𝜇hot = 𝜇cold\n",
        "\n",
        "Alternate Hypothesis :  H₁:\n",
        "𝜇\n",
        "hot\n",
        "≠\n",
        "𝜇\n",
        "cold\n",
        "\n",
        "Test Type: Two-sample t-test"
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from scipy import stats\n",
        "\n",
        "# split the data into the hot and cold weather\n",
        "hot_weather = dataset[dataset['Temperature(°C)'] >= 20]['Rented Bike Count']\n",
        "cold_weather = dataset[dataset['Temperature(°C)'] < 20]['Rented Bike Count']\n",
        "\n",
        "print(\"cold weather bike demand variance :\", np.var(cold_weather))\n",
        "print(\"hot weather bike demand variance :\", np.var(hot_weather))\n"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample size for different weather groups\n",
        "print(\"cold weather sample size :\", len(cold_weather))\n",
        "print(\"hot weather sample size :\", len(hot_weather))"
      ],
      "metadata": {
        "id": "KM7YDJlMgJx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform the t-test\n",
        "t_statistic, p_value = stats.ttest_ind(hot_weather, cold_weather, equal_var=False)\n",
        "\n",
        "print(\"t-statistic:\", t_statistic)\n",
        "if p_value < 0.05:\n",
        "    print(f\"Since p-value ({p_value}) is less than 0.05, we reject null hypothesis.\\nHence, There is a significant difference in mean bike rentals between the 'hot' and 'cold' temperature groups.\")\n",
        "else:\n",
        "    print(f\"Since p-value ({p_value}) is greater than 0.05, we fail to reject null hypothesis.\\nHence, There is no significant difference in mean bike rentals between the 'hot' and 'cold' temperature groups.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "apAYF2yBglh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used Two sample T-test as the statistical testing to obtain P-Value and found the result that Null hypothesis has been rejected and Mean Rented Bike counts different in hot temperatures and cold temperaures."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The two sample t-test is used to determine if there is a significant difference between the means of two groups, making it an appropriate test for comparing the mean number of Rented Bike Count between the hot and cold temperature groups.\n",
        "\n",
        "Also We know from previous charts that Rented Bike Count is right skewed with large sample sizes (i.e.,cold weather sample size : 5832\n",
        " &hot weather bike demand variance : 505140\n",
        ") and we don't know\n",
        "."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2\n",
        "\n",
        "Rented Bike Demand during rush hour (7-9AM & 5-7PM) is higher compared to non-rush hour."
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis: H₀:𝜇hot = 𝜇cold\n",
        "\n",
        "Alternate Hypothesis : H₁: 𝜇 hot ≠ 𝜇 cold\n",
        "\n",
        "Test Type: Two-sample t-test"
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "# Create subsets of the data based on hour\n",
        "rush_hour = dataset[(dataset['Hour'] >= 7) & (dataset['Hour'] <= 9) | (dataset['Hour'] >= 17) & (dataset['Hour'] <= 19)]['Rented Bike Count']\n",
        "non_rush_hour = dataset[~((dataset['Hour'] >= 7) & (dataset['Hour'] <= 9) | (dataset['Hour'] >= 17) & (dataset['Hour'] <= 19))]['Rented Bike Count']\n",
        "\n",
        "print(\"Rush Hour Bike Demand Variance: \", np.var(rush_hour))\n",
        "print(\"Non-Rush Hour Bike Demand Variance: \", np.var(non_rush_hour))"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy\n",
        "# Conduct a two-sample t-test to compare the mean bike rental demand during rush hour with the mean bike rental demand during non-rush hour times\n",
        "t_stat, p_val = scipy.stats.ttest_ind(rush_hour, non_rush_hour, equal_var=False)\n",
        "\n",
        "# Print the t-test results\n",
        "# print('t-statistic:', t_stat)\n",
        "# print('p-value:', p_val)\n",
        "\n",
        "if p_val < 0.05:\n",
        "    print(f\"Since p-value ({p_val}) is less than 0.05, we reject null hypothesis.\\nHence, There is a significant difference in mean bike rentals between the 'rush hour' and 'non-rush hour' times of day.\")\n",
        "else:\n",
        "    print(f\"Since p-value ({p_val}) is greater than 0.05, we fail to reject null hypothesis.\\nHence, There is no significant difference in mean bike rentals between the 'rush hour' and 'non-rush hour' times of day.\")\n"
      ],
      "metadata": {
        "id": "w1ECy80H9lxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used Two sample T-test as the statistical testing to obtain P-Value and found the result that Null hypothesis has been rejected and Mean Rented Bike counts different in rush hours and non-rush hours."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The two sample t-test is used to determine if there is a significant difference between the means of two groups, making it an appropriate test for comparing the mean number of Rented Bike Count between the rush hours and non-rush hours.\n",
        "\n",
        "Also We know from previous charts that Rented Bike Count is right skewed with large sample sizes (i.e.Rush Hour Bike Demand Variance:  651191,\n",
        " & Non-Rush Hour Bike Demand Variance:  294088\n",
        ") and we don't know\n",
        "."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3\n",
        "\n",
        "\n",
        "Rented Bike Demand is different in different seasons with highest in summer and lowest in winter.\n",
        "\n"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Null Hypothesis H₀:\n",
        " No significant difference between rented bike counts for different seasons.\n",
        "\n",
        "Alternate Hypothesis H₁ :\n",
        " Significant difference between rented bike counts for different seasons.\n",
        "\n",
        "Test Type: One-way ANOVA test"
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "# sample size for different seasons\n",
        "dataset.groupby('Seasons')['Rented Bike Count'].count()"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# group the data by season and calculate the mean number of bike rentals for each season\n",
        "season_means = dataset.groupby('Seasons')['Rented Bike Count'].mean()\n",
        "\n",
        "# conduct the ANOVA test\n",
        "f_stat, p_value = scipy.stats.f_oneway(dataset.loc[dataset['Seasons']=='Spring', 'Rented Bike Count'],\n",
        "                                  dataset.loc[dataset['Seasons']=='Summer', 'Rented Bike Count'],\n",
        "                                  dataset.loc[dataset['Seasons']=='Autumn', 'Rented Bike Count'],\n",
        "                                  dataset.loc[dataset['Seasons']=='Winter', 'Rented Bike Count'])\n",
        "\n",
        "# Print the results\n",
        "print('F-statistic:', f_stat)\n",
        "print('p-value:', p_val)\n",
        "print()\n",
        "\n",
        "# Conduct Tukey's HSD test for detailed difference b/w each groups\n",
        "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
        "tukey_results = pairwise_tukeyhsd(dataset['Rented Bike Count'], dataset['Seasons'])\n",
        "\n",
        "# Print the Tukey HSD test results\n",
        "print(tukey_results)"
      ],
      "metadata": {
        "id": "M-zHiv0xAoeg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used One-way ANOVA test as the statistical testing to obtain P-Value and found the result that Null hypothesis has been rejected and Mean Rented Bike counts are significantly different in different seasons."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The one-way ANOVA test is used to determine if there is a significant difference between the means of more than two groups, making it an appropriate test for comparing the mean number of Rented Bike Count between different seasons.\n",
        "\n",
        "Also We know from previous charts that Rented Bike Count is right skewed with large sample sizes ."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "dataset.isnull().sum().sum()"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are no missing value in this dataset."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''# Handling Outliers & Outlier treatments\n",
        "# Removing outliers by using IQR method:\n",
        "q1, q3, median = dataset['Rented Bike Count'].quantile([0.25,0.75,0.5])\n",
        "lower_limit = q1 - 1.5*(q3-q1)\n",
        "upper_limit = q3 + 1.5*(q3-q1)\n",
        "dataset['Rented Bike Count'] = np.where(dataset['Rented Bike Count'] > upper_limit, median,np.where(dataset['Rented Bike Count'] < lower_limit,median,dataset['Rented Bike Count']))\n",
        "\n",
        "# Removing outliers by Capping:\n",
        "for col in ['Wind speed (m/s)','Solar Radiation (MJ/m2)','Rainfall(mm)','Snowfall (cm)']:\n",
        "  upper_limit = dataset[col].quantile(0.99)\n",
        "  dataset[col] = np.where(dataset[col] > upper_limit, upper_limit, dataset[col])'''"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here i used IQR method and Capping method, Based on IQR method i set Upper limit and Lower limit of rented bike count and convert those outliers into median values.\n",
        "\n",
        "Also i have capp outliers upto 99th percentile and above that i convert those outliers into upper limit value.\n",
        "\n",
        "### Note :-\n",
        "I have tried to remove the outliers but it has seen that there is drop in performance after removing the outliers around 10% drop in model performance\n",
        "So, i have decided that i will perform the model without removing the outliers."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "dataset['Snowfall (cm)'] = dataset['Snowfall (cm)'].apply(lambda x: 1 if x>0 else 0)\n",
        "dataset['Rainfall(mm)'] = dataset['Rainfall(mm)'].apply(lambda x: 1 if x>0 else 0)\n",
        "dataset['Visibility (10m)'] = dataset['Visibility (10m)'].apply(lambda x: 0 if x> 399 else (1 if 400<=x<=999 else 2))\n",
        "dataset['Holiday'] = np.where(dataset['Holiday'] == 'Holiday',1,0)\n",
        "dataset['Functioning Day'] = np.where(dataset['Functioning Day'] == 'Yes',1,0)\n"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# One hot encoding\n",
        "dataset = pd.get_dummies(dataset, columns=[ 'Month', 'Hour', 'Day of week','Visibility (10m)'])"
      ],
      "metadata": {
        "id": "GyiW0eJabCqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.columns"
      ],
      "metadata": {
        "id": "eZ8jowA9L5dc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since there are very few day on which there was snowfall / rainfall, it is in my interest that i convert these columns to binary categorical columns indicating whether there was rainfall / snowfall at that particular hour\n",
        "\n",
        "For visibility\n",
        "\n",
        "When\n",
        "\n",
        "Visibility >= 20 Km ---> Clear (high visibility)\n",
        "\n",
        "4 Km <= Visibility < 10 Km ---> Haze (medium visibility)\n",
        "\n",
        "Visibility < 4 Km ---> Fog (low visibility)\n",
        "\n",
        "Converting visibility based on the above mentioned threshold values. Since they are ordinal, we can encode them as 0 (low visibility), 1 (medium visibility), 2 (high visibility)\n",
        "\n",
        "For func day and holiday There are two categories whether its holiday or func day so we use 0 and 1 for that.\n",
        "\n",
        "For Hour, visibility, month & day of the week we use here one hot encoding."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "\n",
        "# We see that the temperature and dew temperature are highly correlated\n",
        "\n",
        "# Plotting Scatter plot to visualize the relationship between\n",
        "# temperature and dew point temperature\n",
        "plt.figure(figsize=(9,6))\n",
        "sns.scatterplot(x='Temperature(°C)',y='Dew point temperature(°C)',data=dataset)\n",
        "plt.xlabel('Temperature(°C)')\n",
        "plt.ylabel('Dew point temperature(°C)')\n",
        "plt.title('Temperature(°C) VS Dew point temperature(°C)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# correlection\n",
        "dataset[['Temperature(°C)', 'Dew point temperature(°C)']].corr()"
      ],
      "metadata": {
        "id": "boN9AbAYRfGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating new temperature column with 50% of both temp\n",
        "dataset['Temperature'] = (dataset['Temperature(°C)']+dataset['Dew point temperature(°C)'])/2"
      ],
      "metadata": {
        "id": "6yJb-loxSXmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting\n",
        "features = [i for i in dataset.columns if i not in ['Rented Bike Count','Temperature(°C)', 'Dew point temperature(°C)']]\n",
        "features"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove multicollinearity by using VIF technique\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "def calc_vif(X):\n",
        "\n",
        "    # Calculating VIF\n",
        "    vif = pd.DataFrame()\n",
        "    vif[\"variables\"] = X.columns\n",
        "    vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
        "\n",
        "    return(vif)"
      ],
      "metadata": {
        "id": "4T-Y9ZFSWk-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "continuous_variables = ['Temperature(°C)', 'Humidity(%)', 'Wind speed (m/s)', 'Dew point temperature(°C)', 'Solar Radiation (MJ/m2)', 'Temperature']"
      ],
      "metadata": {
        "id": "gnk8aNmOWxtw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "continuous_feature_df = pd.DataFrame(dataset[continuous_variables])\n",
        "continuous_feature_df"
      ],
      "metadata": {
        "id": "kfAjl7GSXa8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calc_vif(dataset[[i for i in continuous_feature_df]])"
      ],
      "metadata": {
        "id": "ZapP1lV_X6aw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing Temperature and dew point temperature\n",
        "calc_vif(dataset[[i for i in continuous_feature_df if i not in ['Dew point temperature(°C)','Temperature(°C)']]])\n"
      ],
      "metadata": {
        "id": "oTYycxMVYIGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping data, weekend, temperature and dew_point_temperature\n",
        "dataset.drop(['Dew point temperature(°C)', 'Temperature(°C)','Seasons', 'Date', 'weekend'],axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "xnqGM8NsYnIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.head()"
      ],
      "metadata": {
        "id": "hFxl06WgZ8FJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used pearson correlation coefficient to check correlation between variables and also with dependent variable.\n",
        "\n",
        "And also i check the multicollinearity using VIF and remove those who are having high VIF value."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From above methods i have found that there is high correlation between temperature and dew point temperature. So, i take 50 % of the both and create new variable 'temprature' by adding both of them."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data\n",
        "# Visualizing the distribution of the dependent variable - rental bike count\n",
        "plt.figure(figsize=(9,5))\n",
        "sns.displot(dataset[dependent_variable])\n",
        "plt.xlabel('Rented Bike Count')\n",
        "plt.title('Distribution of Rented Bike Count')\n",
        "plt.axvline(dataset[dependent_variable[0]].mean(), color='red', linestyle='dashed', linewidth=2)\n",
        "plt.axvline(dataset[dependent_variable[0]].median(), color='green', linestyle='dashed', linewidth=2)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Skew of the dependent variable\n",
        "dataset[dependent_variable[0]].skew()"
      ],
      "metadata": {
        "id": "hcMXRZHdng2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the distribution of dependent variable after log transformation\n",
        "plt.figure(figsize=(9,5))\n",
        "sns.displot(np.log(dataset[dependent_variable[0]]))\n",
        "plt.xlabel('Rented Bike Count')\n",
        "plt.title('Distribution of Rented Bike Count')\n",
        "plt.axvline(np.log(dataset['Rented Bike Count']).mean(), color='red', linestyle='dashed', linewidth=2)\n",
        "plt.axvline(np.log(dataset['Rented Bike Count']).median(), color='green', linestyle='dashed', linewidth=2)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rZ1tMVbtnoiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Skew of the dependent variable after log transformation\n",
        "np.log(dataset[dependent_variable[0]]).skew()"
      ],
      "metadata": {
        "id": "9KHWeu9Dn7qC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the distribution of dependent variable after sqrt transformation\n",
        "plt.figure(figsize=(9,5))\n",
        "sns.displot(np.sqrt(dataset[dependent_variable[0]]))\n",
        "plt.xlabel('Rented Bike Count')\n",
        "plt.title('Distribution of Rented Bike Count')\n",
        "plt.axvline(np.sqrt(dataset[dependent_variable[0]]).mean(), color='red', linestyle='dashed', linewidth=2)\n",
        "plt.axvline(np.sqrt(dataset[dependent_variable[0]]).median(), color='green', linestyle='dashed', linewidth=2)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8sI7f7JPoCpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Skew of the dependent variable after sqrt transformation\n",
        "np.sqrt(dataset[dependent_variable[0]]).skew()"
      ],
      "metadata": {
        "id": "iZNhOLZuptfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining dependent and independent variables\n",
        "X = dataset.drop('Rented Bike Count',axis=1)\n",
        "y = np.sqrt(dataset[dependent_variable])"
      ],
      "metadata": {
        "id": "BfP8j9wNp0lE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features"
      ],
      "metadata": {
        "id": "s6W9rwFMqCLA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###I have ploted distribution plot and also i did normality test and i have found that the data is not normally distributed, it needs transformation.\n",
        "\n",
        "So, first i have calculate the skewness value and i have found that the rented bike attribute is positively skewed so i used log transfomation but it affected negatively.\n",
        "\n",
        "So, i finally used square root transformation & now the data looks normally distrubuted & skewness is also reduced."
      ],
      "metadata": {
        "id": "4mSm0WIKqLgE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features = [i for i in dataset.columns if i not in ['Rented Bike Count']]"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(dataset[features])"
      ],
      "metadata": {
        "id": "txfI4vlYf8VG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?\n",
        "\n",
        "\n",
        "In this i have different independent features of different scale so i have used standard scalar method to scale our independent features into one scale."
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Processed dataset shape(rows, columns)\n",
        "dataset.shape"
      ],
      "metadata": {
        "id": "Wbd6zRker1Qv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With 57 columns (independent features) and 8760 rows, and after doing all the feature engineering steps like removing multicolinearity, feature selection manupulations etc. I don't think I need dimensionality reduction here.\n",
        "\n",
        "Essentially where high dimensions are a problem or where it is a particular point in the algorithm to dimension reduction."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size = 0.2, random_state = 0)"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To train the model i have split the data into train and test using train_test_split method.\n",
        "\n",
        "There are two competing concerns: with less training data, your parameter estimates have greater variance. With less testing data, your performance statistic will have greater variance. Broadly speaking you should be concerned with dividing data such that neither variance is too high, which is more to do with the absolute number of instances in each category rather than the percentage.\n",
        "\n",
        "If we have a total of 100 instances, we should probably stick with cross validation as no single split is going to give you satisfactory variance in our estimates. If we have 100,000 instances, it doesn't really matter whether we choose an 80:20 split or a 90:10 split.\n",
        "\n",
        "It is surprising to find out that 80/20 is quite a commonly occurring ratio, often referred to as the Pareto principle.\n",
        "\n",
        "So, in this case i have split 80% of the data into train and 20% into test."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Distribution of Rented Bike Count (target variable)\n",
        "_ = sns.displot(y, kde=True)\n",
        "plt.xlabel('Rented Bike Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8OKt60aGuLsz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at the distribution of traget variable (i.e., Rented Bike Count), values are not concentrated in a narrow range, and is normally distributed accross wide range of values. So the dataset is not imbalanced."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining a function to print evaluation matrix\n",
        "def evaluate_model(model, y_test, y_pred):\n",
        "\n",
        "  '''takes model, y test and y pred values to print evaluation metrics, plot the actual and predicted values,\n",
        "  plot the top 20 important features, and returns a list of the model scores'''\n",
        "\n",
        "  # Squring the y test and and pred as we have used sqrt transformation\n",
        "  y_t = np.square(y_test)\n",
        "  y_p = np.square(y_pred)\n",
        "  y_train2 = np.square(y_train)\n",
        "  y_train_pred = np.square(model.predict(X_train))\n",
        "\n",
        "  # Calculating Evaluation Matrix\n",
        "  mse = mean_squared_error(y_t,y_p)\n",
        "  rmse = np.sqrt(mse)\n",
        "  mae = mean_absolute_error(y_t,y_p)\n",
        "  r2_train = r2_score(y_train2, y_train_pred)\n",
        "  r2 = r2_score(y_t,y_p)\n",
        "  r2_adjusted = 1-(1-r2)*((len(X_test)-1)/(len(X_test)-X_test.shape[1]-1))\n",
        "\n",
        "  # Printing Evaluation Matrix\n",
        "  print(\"MSE :\" , mse)\n",
        "  print(\"RMSE :\" ,rmse)\n",
        "  print(\"MAE :\" ,mae)\n",
        "  print(\"Train R2 :\" ,r2_train)\n",
        "  print(\"Test R2 :\" ,r2)\n",
        "  print(\"Adjusted R2 : \", r2_adjusted)\n",
        "\n",
        "\n",
        "  # plot actual and predicted values\n",
        "  plt.figure(figsize=(13,4))\n",
        "  plt.plot((y_p)[:100])\n",
        "  plt.plot((np.array(y_t)[:100]))\n",
        "  plt.legend([\"Predicted\",\"Actual\"])\n",
        "  plt.title('Actual and Predicted Bike Count', fontsize=15)\n",
        "\n",
        "  try:\n",
        "    importance = model.feature_importances_\n",
        "  except:\n",
        "    importance = model.coef_\n",
        "  importance = np.absolute(importance)\n",
        "  if len(importance)==len(features):\n",
        "    pass\n",
        "  else:\n",
        "    importance = importance[0]\n",
        "\n",
        "  # Feature importances\n",
        "  feat = pd.Series(importance, index=features)\n",
        "  plt.figure(figsize=(9,7))\n",
        "  plt.title('Feature Importances (top 20) for '+str(model), fontsize = 15)\n",
        "  plt.xlabel('Relative Importance')\n",
        "  feat.nlargest(20).plot(kind='barh')\n",
        "\n",
        "\n",
        "  model_score = [mse,rmse,mae,r2_train,r2,r2_adjusted]\n",
        "  return model_score"
      ],
      "metadata": {
        "id": "V8Knv8UWuazk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a score dataframe\n",
        "score = pd.DataFrame(index = ['MSE', 'RMSE', 'MAE', 'Train R2', 'Test R2', 'Adjusted R2'])"
      ],
      "metadata": {
        "id": "y8PhTRmtuspY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1 : LinearRegression"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the LinearRegression class\n",
        "reg = LinearRegression()\n",
        "\n",
        "# Fit the linear regression model to the training data\n",
        "reg.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the model\n",
        "y_pred_li = reg.predict(X_test)"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "linear_score = evaluate_model(reg, y_test,y_pred_li)\n",
        "# Evaluation Metric Score chart\n",
        "score['Linear regression'] = linear_score"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score"
      ],
      "metadata": {
        "id": "oGsFVP6DgraU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "reg = LinearRegression()\n",
        "\n",
        "# Define the hyperparameter grid\n",
        "param_grid = {\n",
        "    'fit_intercept': [True, False]}\n",
        "\n",
        "# Perform grid search\n",
        "grid_search = GridSearchCV(reg, param_grid, cv=5, scoring='r2', return_train_score = True)\n",
        "grid_search.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "RF1bmk59gwwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the best parameters and the corresponding score\n",
        "print(\"Best parameters: \", grid_search.best_params_)\n",
        "print(\"Best R2 score: \", grid_search.best_score_)\n"
      ],
      "metadata": {
        "id": "yDFo4KzikQfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the best parameter to train the model\n",
        "best_reg = grid_search.best_estimator_\n",
        "best_reg.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "KJ-RYAFWkgkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on test data\n",
        "y_pred_li2 = best_reg.predict(X_test)"
      ],
      "metadata": {
        "id": "fdxKzONMkkV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "linear_score2 = evaluate_model(best_reg, y_test,y_pred_li2)"
      ],
      "metadata": {
        "id": "f9N9MIKAks2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score['Linear regression tuned'] = linear_score2\n",
        "score"
      ],
      "metadata": {
        "id": "SWRL4FbJkzlS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GridSearchCV is used to find the best hyperparameters for a machine learning model by searching over a specified parameter grid. It helps to ensure that a model is not overfitting or underfitting by evaluating the model's performance using cross-validation techniques. GridSearchCV can save time and resources compared to manually tuning the parameters of a model.\n",
        "\n",
        "To reduce time and effort i have used GridSearchCV."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For training data, I found R2 score of 0.784428 & 88090.659090 and 201.806803 as MSE and MAE respectively.\n",
        "\n",
        "For testing data, I found R2 score of 0.789520 & 88090.659090 and 201.806803 as MSE and MAE respectively.\n",
        "\n",
        "For Both training and testing data, no improvement is seen."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2 : Lasso Regression"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the Lasso Regression class\n",
        "lasso = Lasso()\n",
        "\n",
        "# Fit the lasso regression model to your training data\n",
        "lasso.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the model\n",
        "y_pred_lasso1 = lasso.predict(X_test)"
      ],
      "metadata": {
        "id": "Zo7uPU3llIUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "lasso_score = evaluate_model(lasso, y_test,y_pred_lasso1)\n",
        "# Evaluation Metric Score chart\n",
        "score['Lasso regression'] = lasso_score"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score"
      ],
      "metadata": {
        "id": "8Q-4SNvSlWpb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is seen that using Lasso regression analysis the performance of the model has drop down. So i will try to tuned the model."
      ],
      "metadata": {
        "id": "tukNIg0zlaxx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "lasso = Lasso()\n",
        "\n",
        "# Define the parameters to be optimized & Perform grid search\n",
        "parameters = {'alpha': [1e-15,1e-10,1e-8,1e-5,1e-4,1e-3,1e-2,1,5,10,20,30,40,45,50,55,60,100]}\n",
        "lasso_regressor = GridSearchCV(lasso, parameters, scoring='neg_mean_squared_error', cv=5)\n",
        "\n",
        "# Fitting model\n",
        "lasso_regressor.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting optimum parameters\n",
        "print(\"The optimum alpha value is found out to be :\" ,lasso_regressor.best_params_)\n",
        "print(\"\\nUsing \",lasso_regressor.best_params_, \" the negative mean squared error is: \", lasso_regressor.best_score_)"
      ],
      "metadata": {
        "id": "TQG9OwJkl79Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the Lasso Regression class with best alpha\n",
        "lasso = Lasso(alpha = lasso_regressor.best_params_['alpha'])\n",
        "\n",
        "# Fit the lasso regression model to your training data\n",
        "lasso.fit(X_train, y_train)\n",
        "\n",
        "# Predict the model\n",
        "y_pred_lassocv = lasso.predict(X_test)\n",
        "\n",
        "\n",
        "#Evaluation matrices for Lasso regression\n",
        "lasso2 = evaluate_model(lasso, y_test,y_pred_lassocv)\n",
        "\n",
        "name = 'Lasso with alpha = ' + str(lasso_regressor.best_params_['alpha'])\n",
        "\n",
        "score[name] = lasso2"
      ],
      "metadata": {
        "id": "h-2OwKKFl_z1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GridSearchCV is used to find the best hyperparameters for a machine learning model by searching over a specified parameter grid. It helps to ensure that a model is not overfitting or underfitting by evaluating the model's performance using cross-validation techniques. GridSearchCV can save time and resources compared to manually tuning the parameters of a model.\n",
        "\n",
        "To reduce time and effort i have used GridSearchCV."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "score"
      ],
      "metadata": {
        "id": "F-Grs_EWmQSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After tuning i have seen that there is increse in performance from 52% to 78%"
      ],
      "metadata": {
        "id": "-7ro-FnwmUv7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "RMSE & MSE are measuring the average squared values between the predicted and actual values.\n",
        "\n",
        "Whereas R2 score is measure how well the model fits the data.\n",
        "\n",
        "In a business context, a low RMSE and high R2 score would indicate that the model is making accurate predictions and is a good fit for the data. This would be desirable for a business because it would mean that the model is able to provide useful insights and make accurate predictions about future outcomes."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3 : Ridge Regression"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ridge regressor class\n",
        "ridge = Ridge()\n",
        "\n",
        "# Fit the ridge regression model to your training data\n",
        "ridge.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the model\n",
        "y_pred_ridge1 = ridge.predict(X_test)"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "result = evaluate_model(ridge, y_test,y_pred_ridge1)\n",
        "score['Ridge'] = result"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score"
      ],
      "metadata": {
        "id": "HnB_63LBmw4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Ridge regressor Class\n",
        "ridge = Ridge()\n",
        "\n",
        "# Define the parameters to be optimized & Perform grid search\n",
        "parameters = {'alpha': [1e-15,1e-10,1e-8,1e-5,1e-4,1e-3,1e-2,1,5,10,20,30,40,45,50,55,60,100]}\n",
        "ridge_regressor = GridSearchCV(ridge, parameters, scoring='neg_mean_squared_error', cv=5)\n",
        "\n",
        "# Fitting model\n",
        "ridge_regressor.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting optimum parameters\n",
        "print(\"The best fit alpha value is found out to be :\" ,ridge_regressor.best_params_)\n",
        "print(\"\\nUsing \",ridge_regressor.best_params_, \" the negative mean squared error is: \", ridge_regressor.best_score_)"
      ],
      "metadata": {
        "id": "scbtSIabm80F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initiate ridge with best alpha\n",
        "ridge = Ridge(alpha = ridge_regressor.best_params_['alpha'])\n",
        "\n",
        "# Fit the ridge regression model to your training data\n",
        "ridge.fit(X_train, y_train)\n",
        "\n",
        "# Predict on model\n",
        "y_pred_ridge = ridge.predict(X_test)\n",
        "\n",
        "\n",
        "# Evaluation matrices for Ridge regression\n",
        "result = evaluate_model(ridge, y_test,y_pred_ridge)\n",
        "\n",
        "namer = 'Ridge with alpha = ' + str(ridge_regressor.best_params_['alpha'])\n",
        "\n",
        "score[namer] = result"
      ],
      "metadata": {
        "id": "35qLWdHvnBTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GridSearchCV is used to find the best hyperparameters for a machine learning model by searching over a specified parameter grid. It helps to ensure that a model is not overfitting or underfitting by evaluating the model's performance using cross-validation techniques. GridSearchCV can save time and resources compared to manually tuning the parameters of a model.\n",
        "\n",
        "To reduce time and effort we have used GridSearchCV."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "score"
      ],
      "metadata": {
        "id": "BaRCVUvdnKOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 4 : Extreme Gradient Boosting Regressor"
      ],
      "metadata": {
        "id": "_4N9ygbzqOeN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Extreme Gradient Boosting Regressor class\n",
        "xgb_model = xgb.XGBRegressor(random_state=0,\n",
        "                             objective='reg:squarederror')\n",
        "\n",
        "# Fit the Extreme Gradient Boosting model to the training data\n",
        "xgb_model.fit(X_train,y_train)\n",
        "\n",
        "# Predict on the model\n",
        "y_pred_xgb1 = xgb_model.predict(X_test)"
      ],
      "metadata": {
        "id": "xsUhlXglqNjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "8BKN8h7qqhym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "result = evaluate_model(xgb_model, y_test,y_pred_xgb1)\n",
        "score['Extreme Gradient Boosting Regressor'] = result"
      ],
      "metadata": {
        "id": "uKVStuqsqe_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score"
      ],
      "metadata": {
        "id": "J0eOBNrWqo3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Extreme Gradient Boosting Regressor i have got accuracy around 97% and 92% on train and test data respectively, which is very good till now, than others algorithm.\n",
        "\n",
        "So, lets tune it."
      ],
      "metadata": {
        "id": "ke2V076qqx6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "Xd9GFCDMq1nV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# XG boost model\n",
        "xgb_model = xgb.XGBRegressor(random_state=0,\n",
        "                             objective='reg:squarederror')\n",
        "xgb_params = {'n_estimators':[500],\n",
        "             'min_samples_leaf':np.arange(20,22)}"
      ],
      "metadata": {
        "id": "VcFNB2bbq5VW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform the randomized search\n",
        "xgb_search = RandomizedSearchCV(xgb_model,xgb_params,cv=6,scoring='neg_root_mean_squared_error',n_iter=100, n_jobs=-1)\n",
        "xgb_search.fit(X_train,y_train)\n",
        "xgb_best_params = xgb_search.best_params_"
      ],
      "metadata": {
        "id": "vZ6zlt_Jq-c7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Best parameters for XG boost Model\n",
        "xgb_best_params"
      ],
      "metadata": {
        "id": "bdue1zxQrKIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Building a XG boost model with best parameters\n",
        "xgb_model = xgb.XGBRegressor(n_estimators=xgb_best_params['n_estimators'],\n",
        "                             min_samples_leaf=xgb_best_params['min_samples_leaf'],\n",
        "                             random_state=0)"
      ],
      "metadata": {
        "id": "dvmbw2EKrMvN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting model\n",
        "xgb_model.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "Zqj1o8xWrQiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the model\n",
        "y_pred_xgb = xgb_model.predict(X_test)"
      ],
      "metadata": {
        "id": "zXeq8ZMTrV3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation matrices for XGBRegressor\n",
        "result = evaluate_model(xgb_model, y_test,y_pred_xgb)\n",
        "score['Extreme Gradient Boosting Regressor Tuned'] = result"
      ],
      "metadata": {
        "id": "iUMSXOwKraAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "tTt3Pk9argaA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Randomized search cross-validation (CV) is used to efficiently explore the hyperparameter space of a machine learning model. It works by randomly sampling from the search space of hyperparameters, rather than exhaustively trying every possible combination. This allows for a more efficient search while still providing a good chance of finding good hyperparameter values. Additionally, by using cross-validation to evaluate the performance of each set of hyperparameters, one can ensure that the model is not overfitting to the training data.\n",
        "\n",
        "Because of its randomly sampling technique and to save the time i have decided to use Randomized search CV."
      ],
      "metadata": {
        "id": "k-lm0WaBriBF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "PXHgZKkJrrN5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "score"
      ],
      "metadata": {
        "id": "iUgfwJnQrdup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After tuning the model i have got accuracy on train data arround 99 % and 93% on test data which is very good model accuracy, like Gradient Boosting Tuning data. But it is overfitting the model's training accuracy is almost 100%.\n",
        "\n",
        "So, i would like to go with the model accuracy data of XGboost, before tuning. Which is 97% on train data and 92% on test data."
      ],
      "metadata": {
        "id": "wMFqDMovryku"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Plot R2 scores for each model**"
      ],
      "metadata": {
        "id": "DpcWFAfLr4oX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "score.columns"
      ],
      "metadata": {
        "id": "UFLTA6c8r3IW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# R2 Scores plot\n",
        "\n",
        "models = list(score.columns)\n",
        "train = score.iloc[-3,:]\n",
        "test = score.iloc[-2,:]\n",
        "\n",
        "X_axis = np.arange(len(models))\n",
        "\n",
        "plt.figure(figsize=(20,8))\n",
        "plt.bar(X_axis - 0.2, train, 0.4, label = 'Train R2 Score')\n",
        "plt.bar(X_axis + 0.2, test, 0.4, label = 'Test R2 Score')\n",
        "\n",
        "\n",
        "plt.xticks(X_axis,models, rotation=30)\n",
        "plt.ylabel(\"R2 Score\")\n",
        "plt.title(\"R2 score for each model\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yKopHcjcsJ61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Plot of adjusted R2**"
      ],
      "metadata": {
        "id": "Q3koCHFGsVGi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing the overfitted models which have more than 5% gap between train and test values\n",
        "score_t = score.transpose()            #taking transpose of the score dataframe to create new difference column\n",
        "score_t['diff']=score_t['Train R2']-score_t['Test R2']                   #creating new column diff of train R2 and test R2 score\n",
        "remove_models = list(score_t[score_t['diff']>=.05].index)                #creating a list of models which have difference more than .05 that is 5%\n",
        "remove_models\n",
        "\n",
        "adj = score_t['Adjusted R2'].drop(remove_models)                     #creating a new dataframe with required models and adjusted r2 score\n",
        "\n",
        "\n",
        "# Visualizing a bar plot for adjusted R2 score\n",
        "plt.figure(figsize=(12,7))\n",
        "plots = sns.barplot(x=list(adj.index), y=adj)\n",
        "for bar in plots.patches:\n",
        "  plots.annotate(format(bar.get_height(),'.2f'),\n",
        "                   (bar.get_x() + bar.get_width() / 2,\n",
        "                    bar.get_height()), ha='center', va='center',\n",
        "                   size=8, xytext=(0, 5),\n",
        "                   textcoords='offset points')\n",
        "plt.xticks(rotation=30)\n",
        "\n",
        "plt.title(\" Adjusted R2 score\", fontsize = 15)\n",
        "plt.xlabel('Models', fontsize = 12)\n",
        "plt.ylabel('Score', fontsize = 12)\n",
        "\n",
        "# Setting limit of the y axis from 0 to 30\n",
        "plt.ylim(0,1)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pHJFlceMsUOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On the basis of all the model i have decided to select R2 score Evaluation matrics which shows the accuracy of the model which is very good indicator to check the feasibility of the model."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have ran a several models like linear regression, decision, and xtreame gradient boosting but amongst them i have selected extreme gradient boosting regressor as i achieved 97% training accuracy and 92% testing accuracy. Some models were overfitted so i did not consider them."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SHAP(Shapley additive Explanations)**"
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "# Shap explainer for xgb model (tree based)\n",
        "explainer = shap.TreeExplainer(xgb_model, X_train, feature_names=features)\n",
        "# Plotting\n",
        "shap.initjs()\n",
        "# Select an instance from the test set\n",
        "instance = X_test[50, :]\n",
        "\n",
        "# Compute the SHAP values for the instance\n",
        "shap_values = explainer(instance)\n",
        "\n",
        "# Create the SHAP force plot\n",
        "shap.plots.force(shap_values)"
      ],
      "metadata": {
        "id": "V8iPvYN-s3Es"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The force plot shows the shap values for a particular instance.\n",
        "\n",
        "Here i have considered the 50th index row values for the plot. I can see that the prediction is 22.54 (sqrt value). The different contribution of the columns is shown for getting the prediction."
      ],
      "metadata": {
        "id": "y7q9snWVuEVJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get shap values of test data\n",
        "shap_values = explainer(X_test)"
      ],
      "metadata": {
        "id": "Q4Jz0WKIt1UC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the SHAP summary plot\n",
        "shap.summary_plot(shap_values, X_test)"
      ],
      "metadata": {
        "id": "sTuQ7-X6uZ_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the summary plot i can see the top 20 columns and their impact on the prediction. The red color indicates that the value of the columns is high and blue color shows that the value of the column is low.\n",
        "\n",
        "For categorical columns, i have zeros and ones where zero is blue color and one is red color.\n",
        "\n",
        "Shap values are also displayed and the impact on the prediction is also shown. Towards the right hand side, the impact is positive (increases the predicted value) and towards the left hand side, the impact is negative (decreases the predicted value)."
      ],
      "metadata": {
        "id": "d6U3i_0rug26"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "# Save the model to a pickle file\n",
        "with open(\"xgb_model.pkl\", \"wb\") as f:\n",
        "  pickle.dump(xgb_model, f)"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data.\n",
        "with open(\"xgb_model.pkl\", \"rb\") as f:\n",
        "    loaded_xgb_model = pickle.load(f)\n",
        "\n",
        "new_test_preds = loaded_xgb_model.predict(X_test)\n",
        "\n",
        "# Sanity Check\n",
        "mse = mean_squared_error(y_test, new_test_preds)\n",
        "mae = mean_absolute_error(y_test, new_test_preds)\n",
        "r2 = r2_score(y_test, new_test_preds)\n",
        "print(\"MSE:\", mse)\n",
        "print(\"MAE:\", mae)\n",
        "print(\"R2 Score:\", r2)"
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The project successfully demonstrated the feasibility of using machine learning techniques to predict bike demand in Seoul.\n",
        "\n",
        "Some of the key points are:-\n",
        "\n",
        "High demand in the morning and evening.\n",
        "Less Demand in the winter season.\n",
        "Highest demand in june.\n",
        "Found multicollinearity between temperature and dew point temperature.\n",
        "Perform linear regression, decision tree, random forest, gradient boosting, Extreme gradient boosting. & got highest accuracy i.e 97% on train and 92% on test on Xtreme gradient boosting.\n",
        "There is no use of removing outliers, it affects negatively on model performance.\n",
        "Overall, the project highlights the potential of machine learning in solving real-world problems and provides a roadmap for future research in this area. The findings of this project can be extended to other cities with similar bike sharing systems, leading to more effective and efficient bike sharing operations, and better outcomes for all stakeholders."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}